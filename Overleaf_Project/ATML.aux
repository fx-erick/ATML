\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{british}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{lee2020network}
\citation{kirk2021survey}
\citation{kirk2021survey}
\citation{kirk2021survey}
\citation{kirk2021survey}
\citation{agarwal2021contrastive}
\citation{kirk2021survey}
\citation{agarwal2021contrastive}
\citation{hadeep}
\citation{hadeep}
\citation{kirk2021survey}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{}{{\caption@xref {}{ on input line 65}}{2}{Introduction}{subfigure.1.1}{}}
\newlabel{}{{\caption@xref {}{ on input line 70}}{2}{Introduction}{subfigure.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Train and Test task where agent fails\relax }}{2}{figure.caption.1}\protected@file@percent }
\newlabel{fig:TrainTestTask}{{1}{2}{Train and Test task where agent fails\relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Theory and Concepts}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Generalization in RL}{2}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}State representation in RL}{2}{subsection.2.2}\protected@file@percent }
\newlabel{sec: state repr}{{2.2}{2}{State representation in RL}{subsection.2.2}{}}
\citation{github_contrastive_similarity}
\citation{github_contrastive_similarity}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Categorization of current research on generalization in RL according to \cite  {kirk2021survey}. The studied paper \cite  {agarwal2021contrastive} is an instance of learning invariances. \relax }}{3}{figure.caption.2}\protected@file@percent }
\newlabel{fig: overview_research}{{2}{3}{Categorization of current research on generalization in RL according to \cite {kirk2021survey}. The studied paper \cite {agarwal2021contrastive} is an instance of learning invariances. \relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}The $\pi $-bisimulation and Policy Similarity Metric}{3}{subsection.2.3}\protected@file@percent }
\newlabel{sec: PSM}{{2.3}{3}{The $\pi $-bisimulation and Policy Similarity Metric}{subsection.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Understanding behavioral similarity. The agent needs to obtain the reward (spinach) while maintaining distance from the virus. Even though the initial states are visually different, they are similar in terms of their optimal behavior at current states as well as future states following the current state. Policy similarity metric (PSM) assigns high similarity to such behaviorally similar states and low similarity to dissimilar states.\cite  {github_contrastive_similarity} \relax }}{3}{figure.caption.3}\protected@file@percent }
\newlabel{fig:BehavioralSimilarity}{{3}{3}{Understanding behavioral similarity. The agent needs to obtain the reward (spinach) while maintaining distance from the virus. Even though the initial states are visually different, they are similar in terms of their optimal behavior at current states as well as future states following the current state. Policy similarity metric (PSM) assigns high similarity to such behaviorally similar states and low similarity to dissimilar states.\cite {github_contrastive_similarity} \relax }{figure.caption.3}{}}
\citation{puterman2005markov}
\citation{agarwal2021contrastive}
\citation{castro2020scalable}
\citation{pscsWebsite}
\citation{pscsWebsite}
\newlabel{}{{\caption@xref {}{ on input line 167}}{4}{The $\pi $-bisimulation and Policy Similarity Metric}{subfigure.4.1}{}}
\newlabel{}{{\caption@xref {}{ on input line 172}}{4}{The $\pi $-bisimulation and Policy Similarity Metric}{subfigure.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Bisimulation equivalence relation. The 8 states on the image on the left would collapse to an equivalent 4-states MDP \cite  {pscsWebsite}\relax }}{4}{figure.caption.4}\protected@file@percent }
\newlabel{fig:BisimulationEquivalence}{{4}{4}{Bisimulation equivalence relation. The 8 states on the image on the left would collapse to an equivalent 4-states MDP \cite {pscsWebsite}\relax }{figure.caption.4}{}}
\citation{castro2020scalable}
\citation{castro2020scalable}
\citation{slidelive}
\citation{agarwal2021contrastive}
\citation{agarwal2021contrastive}
\citation{slidelive}
\citation{github_contrastive_similarity}
\citation{github_contrastive_similarity}
\citation{castro2020scalable}
\citation{castro2020scalable}
\citation{castro2020scalable}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Same policies different rewards: Cyan edges represent optimal actions with a positive reward. The rest have zero reward. $x_0$, $y_0$ are the start states and $x_2$, $y_2$ denote the terminal states \cite  {agarwal2021contrastive}\relax }}{5}{figure.caption.5}\protected@file@percent }
\newlabel{fig:TwoMDP}{{5}{5}{Same policies different rewards: Cyan edges represent optimal actions with a positive reward. The rest have zero reward. $x_0$, $y_0$ are the start states and $x_2$, $y_2$ denote the terminal states \cite {agarwal2021contrastive}\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Same reward different policies: he three states with a yellow boundary have the same expected rewards, which means that the bisimulation value is 0. However, the exhibit different behaviour. \cite  {github_contrastive_similarity}\relax }}{5}{figure.caption.6}\protected@file@percent }
\newlabel{fig:ThreeJump}{{6}{5}{Same reward different policies: he three states with a yellow boundary have the same expected rewards, which means that the bisimulation value is 0. However, the exhibit different behaviour. \cite {github_contrastive_similarity}\relax }{figure.caption.6}{}}
\citation{castro2020scalable}
\citation{github_contrastive_similarity}
\citation{github_contrastive_similarity}
\citation{castro2020scalable}
\citation{castro2020scalable}
\citation{castro2020scalable}
\citation{tian2021understanding}
\citation{Hadsell06dimensionalityreduction}
\newlabel{eq:BisimulationDistance}{{2}{6}{The $\pi $-bisimulation and Policy Similarity Metric}{equation.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Sand Pile analogy of Wasserstein metric \cite  {github_contrastive_similarity}\relax }}{6}{figure.caption.7}\protected@file@percent }
\newlabel{fig:SandPile}{{7}{6}{Sand Pile analogy of Wasserstein metric \cite {github_contrastive_similarity}\relax }{figure.caption.7}{}}
\citation{chen2020simple}
\citation{chen2020simple}
\citation{agarwal2021contrastive}
\citation{agarwal2021contrastive}
\citation{agarwal2021contrastive}
\citation{agarwal2021contrastive}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Policy Similarity Embeddings - PSEs}{7}{subsection.2.4}\protected@file@percent }
\newlabel{Gaussian Kernel}{{4}{7}{Policy Similarity Embeddings - PSEs}{equation.2.4}{}}
\newlabel{contrastive loss}{{5}{7}{Policy Similarity Embeddings - PSEs}{equation.2.5}{}}
\newlabel{eq: total_loss}{{6}{7}{Policy Similarity Embeddings - PSEs}{equation.2.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Experiments from the paper}{7}{section.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces An input pair (x,y) is first augmented and then mapped into the representation space $f$. The loss function defined in Equation \ref  {eq: total_loss} is applied to a non-linear projection $z$ of this representation space. \cite  {agarwal2021contrastive} \relax }}{8}{figure.caption.8}\protected@file@percent }
\newlabel{fig: Learning_Architecture}{{8}{8}{An input pair (x,y) is first augmented and then mapped into the representation space $f$. The loss function defined in Equation \ref {eq: total_loss} is applied to a non-linear projection $z$ of this representation space. \cite {agarwal2021contrastive} \relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Jumping Task from Pixels: A Case Study}{8}{subsection.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Optimal trajectories are shown for two different environments. The trajectory is a sequence of right actions interrupted by a single jump action. \cite  {agarwal2021contrastive} \relax }}{8}{figure.caption.9}\protected@file@percent }
\newlabel{fig: optimal_trajectories}{{9}{8}{Optimal trajectories are shown for two different environments. The trajectory is a sequence of right actions interrupted by a single jump action. \cite {agarwal2021contrastive} \relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Classification of the problem}{8}{subsection.3.2}\protected@file@percent }
\citation{lee2020network}
\citation{agarwal2021contrastive}
\citation{agarwal2021contrastive}
\citation{agarwal2021contrastive}
\citation{agarwal2021contrastive}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Results from the paper}{9}{subsection.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The table summarizes the main results of the authors, giving percentages of how many test tasks could be solved using different methods with or without data augmentation. The grid configurations are shown in Figure \ref  {fig: grid_results}. It can be observed that PSEs outperform the compared methods in most configurations except for the narrow grid without data augmentation. Also, PSEs profit substantially from data augmentation. \cite  {agarwal2021contrastive}\relax }}{9}{figure.caption.10}\protected@file@percent }
\newlabel{fig: tabular_results}{{10}{9}{The table summarizes the main results of the authors, giving percentages of how many test tasks could be solved using different methods with or without data augmentation. The grid configurations are shown in Figure \ref {fig: grid_results}. It can be observed that PSEs outperform the compared methods in most configurations except for the narrow grid without data augmentation. Also, PSEs profit substantially from data augmentation. \cite {agarwal2021contrastive}\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces The red letter T shows which are the training tasks for the three grid configurations and the background colour of each tile shows the performance of PSEs across the task configurations, which is reported as the median of 100 runs per configurations using data augmentation. Beige tiles correspond to tasks that could be solved and black tiles could not be solved. \cite  {agarwal2021contrastive}\relax }}{9}{figure.caption.11}\protected@file@percent }
\newlabel{fig: grid_results}{{11}{9}{The red letter T shows which are the training tasks for the three grid configurations and the background colour of each tile shows the performance of PSEs across the task configurations, which is reported as the median of 100 runs per configurations using data augmentation. Beige tiles correspond to tasks that could be solved and black tiles could not be solved. \cite {agarwal2021contrastive}\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results of reproducibility study}{10}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Jumping Task from Pixels : Main result}{10}{subsection.4.1}\protected@file@percent }
\newlabel{}{{\caption@xref {}{ on input line 375}}{11}{Jumping Task from Pixels : Main result}{subfigure.12.1}{}}
\newlabel{}{{\caption@xref {}{ on input line 380}}{11}{Jumping Task from Pixels : Main result}{subfigure.12.2}{}}
\newlabel{}{{\caption@xref {}{ on input line 385}}{11}{Jumping Task from Pixels : Main result}{subfigure.12.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Test performance curves in the setting \textbf  {without data augmentation} on the wide, narrow, and random grids. We plot the median performance across 5 runs. Shaded regions show 25 and 75 percentiles\relax }}{11}{figure.caption.12}\protected@file@percent }
\newlabel{fig:ResultWithoutDataAug}{{12}{11}{Test performance curves in the setting \textbf {without data augmentation} on the wide, narrow, and random grids. We plot the median performance across 5 runs. Shaded regions show 25 and 75 percentiles\relax }{figure.caption.12}{}}
\newlabel{}{{\caption@xref {}{ on input line 396}}{11}{Jumping Task from Pixels : Main result}{subfigure.13.1}{}}
\newlabel{}{{\caption@xref {}{ on input line 401}}{11}{Jumping Task from Pixels : Main result}{subfigure.13.2}{}}
\newlabel{}{{\caption@xref {}{ on input line 406}}{11}{Jumping Task from Pixels : Main result}{subfigure.13.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Test performance curves in the setting \textbf  {with data augmentation} on the wide, narrow, and random grids. We plot the median performance across 5 runs. Shaded regions show 25 and 75 percentiles\relax }}{11}{figure.caption.13}\protected@file@percent }
\newlabel{fig:ResultWithDataAug}{{13}{11}{Test performance curves in the setting \textbf {with data augmentation} on the wide, narrow, and random grids. We plot the median performance across 5 runs. Shaded regions show 25 and 75 percentiles\relax }{figure.caption.13}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Percentage (\%) of test tasks solved by different methods without and with data augmentation. The wide, narrow, and random grids. We report average performance across 5 runs with different random initializations, with standard deviation between parentheses\relax }}{12}{table.caption.14}\protected@file@percent }
\newlabel{table:Rec_bisection}{{1}{12}{Percentage (\%) of test tasks solved by different methods without and with data augmentation. The wide, narrow, and random grids. We report average performance across 5 runs with different random initializations, with standard deviation between parentheses\relax }{table.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Jumping task with red and green colored obstacles. The optimal actions are jumping over the red obstacle and hitting the green obstacle.\relax }}{12}{figure.caption.15}\protected@file@percent }
\newlabel{fig:JumpingColors}{{14}{12}{Jumping task with red and green colored obstacles. The optimal actions are jumping over the red obstacle and hitting the green obstacle.\relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Jumping Task from Pixels : Hyperparameter Sensitivity tests}{12}{subsection.4.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Hyperparameters used for the jumping task with Rand Conv + PSE in wide training grid\relax }}{13}{table.caption.16}\protected@file@percent }
\newlabel{}{{\caption@xref {}{ on input line 475}}{13}{Jumping Task from Pixels : Hyperparameter Sensitivity tests}{subfigure.15.1}{}}
\newlabel{}{{\caption@xref {}{ on input line 480}}{13}{Jumping Task from Pixels : Hyperparameter Sensitivity tests}{subfigure.15.2}{}}
\newlabel{}{{\caption@xref {}{ on input line 485}}{13}{Jumping Task from Pixels : Hyperparameter Sensitivity tests}{subfigure.15.3}{}}
\newlabel{}{{\caption@xref {}{ on input line 490}}{13}{Jumping Task from Pixels : Hyperparameter Sensitivity tests}{subfigure.15.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Results of hyperparameter sensitivity test for jumping task with random convolution + PSE and wide training grid.\relax }}{13}{figure.caption.17}\protected@file@percent }
\newlabel{fig:HyperParameterTuning}{{15}{13}{Results of hyperparameter sensitivity test for jumping task with random convolution + PSE and wide training grid.\relax }{figure.caption.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Distracting DM Control Tasks}{14}{subsection.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Visualization of the distracting DM control suite. The model is trained in 6 different environments with video distractions. It is then evaluated in the same environments but with different video distractions. \relax }}{14}{figure.caption.18}\protected@file@percent }
\newlabel{fig:DM}{{16}{14}{Visualization of the distracting DM control suite. The model is trained in 6 different environments with video distractions. It is then evaluated in the same environments but with different video distractions. \relax }{figure.caption.18}{}}
\citation{openai}
\citation{openai}
\bibstyle{plain}
\bibdata{citations}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Different Environment Tests}{15}{subsection.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Visualization of the classic gym cartpole environment from OpenAI. The goal of this game is to balance the cartpole and the reward is the total amount of time that the cartpole stays upright.\cite  {openai}\relax }}{15}{figure.caption.19}\protected@file@percent }
\newlabel{fig:Cartpole}{{17}{15}{Visualization of the classic gym cartpole environment from OpenAI. The goal of this game is to balance the cartpole and the reward is the total amount of time that the cartpole stays upright.\cite {openai}\relax }{figure.caption.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Overall Remarks Regarding Reproducibility}{15}{subsection.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Workload Partition among Group Members}{15}{subsection.4.6}\protected@file@percent }
\bibcite{agarwal2021contrastive}{1}
\bibcite{castro2020scalable}{2}
\bibcite{pscsWebsite}{3}
\bibcite{chen2020simple}{4}
\bibcite{hadeep}{5}
\bibcite{Hadsell06dimensionalityreduction}{6}
\bibcite{kirk2021survey}{7}
\bibcite{lee2020network}{8}
\bibcite{openai}{9}
\bibcite{puterman2005markov}{10}
\bibcite{github_contrastive_similarity}{11}
\bibcite{slidelive}{12}
\bibcite{tian2021understanding}{13}
\gdef \@abspage@last{16}
