\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{british}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{lee2020network}
\citation{kirk2021survey}
\citation{kirk2021survey}
\citation{kirk2021survey}
\citation{kirk2021survey}
\citation{agarwal2021contrastive}
\citation{kirk2021survey}
\citation{agarwal2021contrastive}
\citation{hadeep}
\citation{hadeep}
\citation{kirk2021survey}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{}{{\caption@xref {}{ on input line 58}}{2}{Introduction}{subfigure.1.1}{}}
\newlabel{}{{\caption@xref {}{ on input line 63}}{2}{Introduction}{subfigure.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Train and Test task where agent fails\relax }}{2}{figure.caption.1}\protected@file@percent }
\newlabel{fig:TrainTestTask}{{1}{2}{Train and Test task where agent fails\relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Theory and Concepts}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Generalization in RL}{2}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}State representation in RL}{2}{subsection.2.2}\protected@file@percent }
\newlabel{sec: state repr}{{2.2}{2}{State representation in RL}{subsection.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Categorization of current research on generalization in RL according to \cite  {kirk2021survey}. The studied paper \cite  {agarwal2021contrastive} is an instance of learning invariances. \relax }}{3}{figure.caption.2}\protected@file@percent }
\newlabel{fig: overview_research}{{2}{3}{Categorization of current research on generalization in RL according to \cite {kirk2021survey}. The studied paper \cite {agarwal2021contrastive} is an instance of learning invariances. \relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}The $\pi $-bisimulation and Policy Similarity Metric}{3}{subsection.2.3}\protected@file@percent }
\newlabel{sec: PSM}{{2.3}{3}{The $\pi $-bisimulation and Policy Similarity Metric}{subsection.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Behavioral similarity\relax }}{4}{figure.caption.3}\protected@file@percent }
\newlabel{fig:BehavioralSimilarity}{{3}{4}{Behavioral similarity\relax }{figure.caption.3}{}}
\newlabel{}{{\caption@xref {}{ on input line 165}}{4}{The $\pi $-bisimulation and Policy Similarity Metric}{subfigure.4.1}{}}
\newlabel{}{{\caption@xref {}{ on input line 170}}{4}{The $\pi $-bisimulation and Policy Similarity Metric}{subfigure.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Bisimulation equivalence relation\relax }}{4}{figure.caption.4}\protected@file@percent }
\newlabel{fig:BisimulationEquivalence}{{4}{4}{Bisimulation equivalence relation\relax }{figure.caption.4}{}}
\newlabel{eq:BisimulationDistance}{{2}{5}{The $\pi $-bisimulation and Policy Similarity Metric}{equation.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Sand Pile analogy of Wasserstein metric\relax }}{5}{figure.caption.5}\protected@file@percent }
\newlabel{fig:SandPile}{{5}{5}{Sand Pile analogy of Wasserstein metric\relax }{figure.caption.5}{}}
\citation{tian2021understanding}
\citation{Hadsell06dimensionalityreduction}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Same policies different rewards\relax }}{6}{figure.caption.6}\protected@file@percent }
\newlabel{fig:TwoMDP}{{6}{6}{Same policies different rewards\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Same reward different policies\relax }}{6}{figure.caption.7}\protected@file@percent }
\newlabel{fig:ThreeJump}{{7}{6}{Same reward different policies\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Policy Similarity Embeddings - PSEs}{6}{subsection.2.4}\protected@file@percent }
\citation{chen2020simple}
\citation{agarwal2021contrastive}
\citation{agarwal2021contrastive}
\newlabel{Gaussian Kernel}{{4}{7}{Policy Similarity Embeddings - PSEs}{equation.2.4}{}}
\newlabel{contrastive loss}{{5}{7}{Policy Similarity Embeddings - PSEs}{equation.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces An input pair (x,y) is first augmented and then mapped into the representation space $f$. The loss function defined in Equation \ref  {contrastive loss} is applied to a non-linear projection $z$ of this representation space. \cite  {agarwal2021contrastive} \relax }}{7}{figure.caption.8}\protected@file@percent }
\newlabel{fig: Learning_Architecture}{{8}{7}{An input pair (x,y) is first augmented and then mapped into the representation space $f$. The loss function defined in Equation \ref {contrastive loss} is applied to a non-linear projection $z$ of this representation space. \cite {agarwal2021contrastive} \relax }{figure.caption.8}{}}
\citation{agarwal2021contrastive}
\citation{agarwal2021contrastive}
\citation{lee2020network}
\@writefile{toc}{\contentsline {section}{\numberline {3}Experiments from the paper}{8}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Jumping Task from Pixels: A Case Study}{8}{subsection.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Optimal trajectories are shown for two different environments. The trajectory is a sequence of right actions interrupted by a single jump action. \cite  {agarwal2021contrastive} \relax }}{8}{figure.caption.9}\protected@file@percent }
\newlabel{fig: optimal_trajectories}{{9}{8}{Optimal trajectories are shown for two different environments. The trajectory is a sequence of right actions interrupted by a single jump action. \cite {agarwal2021contrastive} \relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Classification of the problem}{8}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Results from the paper}{8}{subsection.3.3}\protected@file@percent }
\citation{agarwal2021contrastive}
\citation{agarwal2021contrastive}
\citation{agarwal2021contrastive}
\citation{agarwal2021contrastive}
\bibstyle{plain}
\bibdata{citations}
\bibcite{agarwal2021contrastive}{1}
\bibcite{chen2020simple}{2}
\bibcite{hadeep}{3}
\bibcite{Hadsell06dimensionalityreduction}{4}
\bibcite{kirk2021survey}{5}
\bibcite{lee2020network}{6}
\bibcite{tian2021understanding}{7}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The table summarizes the main results of the authors, giving percentages of how many test tasks could be solved using different methods with or without data augmentation. The grid configurations are shown in Figure \ref  {fig: grid_results}. It can be observed that PSEs outperform the compared methods in most configurations except for the narrow grid without data augmentation. Also, PSEs profit substantially from data augmentation. \cite  {agarwal2021contrastive}\relax }}{9}{figure.caption.10}\protected@file@percent }
\newlabel{fig: tabular_results}{{10}{9}{The table summarizes the main results of the authors, giving percentages of how many test tasks could be solved using different methods with or without data augmentation. The grid configurations are shown in Figure \ref {fig: grid_results}. It can be observed that PSEs outperform the compared methods in most configurations except for the narrow grid without data augmentation. Also, PSEs profit substantially from data augmentation. \cite {agarwal2021contrastive}\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces The red letter T shows which are the training tasks for the three grid configurations and the background colour of each tile shows the performance of PSEs accross the task configurations, which is reported as the median of 100 runs per configurations using data augmentation. Beige tiles correspond to tasks that could be solved and black tiles could not be solved. \cite  {agarwal2021contrastive}\relax }}{9}{figure.caption.11}\protected@file@percent }
\newlabel{fig: grid_results}{{11}{9}{The red letter T shows which are the training tasks for the three grid configurations and the background colour of each tile shows the performance of PSEs accross the task configurations, which is reported as the median of 100 runs per configurations using data augmentation. Beige tiles correspond to tasks that could be solved and black tiles could not be solved. \cite {agarwal2021contrastive}\relax }{figure.caption.11}{}}
\gdef \@abspage@last{9}
