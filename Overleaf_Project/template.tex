\documentclass[unicode,11pt,a4paper,oneside,numbers=endperiod,openany]{scrartcl}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{caption}
\usepackage{subcaption}
\input{assignment.sty}
\def\code#1{\texttt{#1}}
\usepackage{hyperref} 
\begin{document}


\setassignment

\serieheader{Advanced Techniques in Machine Learning}{2021}{Student: }{}{Reproducibility of <paper name>}{}

\tableofcontents

\section{Introduction}

Reinforcement Learning is used in sequential decision making for training agents in complex tasks. The agent interacts with the environment which, based on the agent’s action, provides the agent with some reward (positive or negative). The agent then uses this feedback to update its behaviour with the goal of being optimal.

Although current RL agents have demonstrated great potential in a variety of activities, they face difficulties in transferring these agent’s capabilities to new unseen tasks. This happens even when the tasks are semantically equivalent. In other words, existing reinforcement learning agents frequently learn policies that do not generalize well to environments different than those environments these agents are trained on.

For example, the paper considered a jumping task, where we have an agent that needs to jump over an obstacle. The agent learns from image observations. In order for the agent to not collide with the obstacle, the agent needs to precisely time the jump at a particular distance from the obstacle. The various tasks consist of shifting the floor height or changing the obstacle position or both. If we train deep RL agents on some of these tasks where we vary the obstacle positions, they perform poorly at previously unseen locations (i.e. struggle to jump over the obstacle without hitting it).

The challenge tackled in the paper is to generalize to unseen positions of the obstacle and floor heights in the test tasks, while using only a finite number of training tasks or environments sampled from a distribution of tasks.

\begin{figure}[h]
    \centering
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width = \linewidth]{ATML/Introduction/image1.png}
        \label{}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width = \linewidth]{ATML/Introduction/image2.png}
        \label{}
    \end{subfigure}
    \caption{Train and Test task where agent fails}
    \label{fig:TrainTestTask}
\end{figure}

A few proposed solutions to poor generalization, adapted from supervised learning, include regularization (such as l2-regularization, dropout or noise injection), Domain Randomization and Data Augmentation (such as RandConv, RAD, DrQ), which is being used more recently. The majority of these ideas centre around improving the learning process in order to fully harness decision-making process, and they rarely use properties of the sequential aspect such as similarity in actions across temporal observations. Instead, this paper takes advantage of the fact that when an agent operates in similar tasks, the agent shows at least short sequences of behaviours that are similar across these tasks.

In their approach, the authors train the agents to learn a representation where the states are close when the optimal behaviour of the agent in the current states and future states are similar. They use the notion of behavioural similarity as it has the property of good generalization to observations across different tasks. This behavioural similarity between states across various tasks is quantified using the Policy Similarity Metric (PSM), which is a state-similarity metric based on bisimulation. Moreover, to enhance generalization, the agent is trained to learn state embeddings. This is done using Contrastive Similarity Metric (CSM).

\section{The $\pi$-bisimulation and Policy Similarity Metric}
In the paper, the authors learn representations that encode behavioural similarity across states. To have a better understanding of the behavioural similarity, think of an agent who needs to reach spinach at the supermarket while maintaining social distancing of 6 feet. In the first environment to the left, the agent takes two actions to the right followed by two actions down. Now we look at the second environment on the right with a slightly different layout but where the agent takes the same actions (two steps right and two steps down) (as seen in figure \ref{fig:BehavioralSimilarity}). This is behavioural similarity where actions in the current state as well as in the future states are similar.

\begin{figure}[h]
    \centering
    \includegraphics[width = \linewidth]{ATML/Introduction/image3.png}
    \caption{Behavioral similarity}
    \label{fig:BehavioralSimilarity}
\end{figure}

To mathematically define the behavioural similarity, we use the concepts of bisimulation metric. Bisimulation metrics quantify the behavioural distance between two states in a Markov decision process. To mathematically define the bisimulation metric, we need some notations.

The Markov Decision Process (MDP) is defined as a 5-tuple M = (S, A, R, P, $\gamma$), where:
\begin{itemize}
    \item S denoted the set of all possible state.
    \item A denotes set of all possible actions.
    \item R denotes a reward function.
    \begin{itemize}
        \item $R(x,a,x')$ is the immediate reward received after $a$ transition from state $x$ to state $x'$ because of action $a$
    \end{itemize}
    \item P denotes the state transition probabilities.
    \begin{itemize}
        \item $P(x,a,x')$ is the probability that action $a$ in state $x$ at time $t$ will lead to a state $x'$ at $t+1$.
    \end{itemize}
    \item $\gamma \in  [0, 1)$  is the discount factor which is used to generate a discounted reward.
\end{itemize}
Each state $x$ encodes sufficient information about the environment such that an agent can learn how to behave in a consistent manner. Policy $\pi$ is a controller that helps us select the actions to maximize expected return $\mathbb{E}_{a_{t}\sim \pi(\cdot|x)} [\sum_t \gamma^t R(x_t,a_t)]$. A policy $\pi$(·$|$x) maps states $x \in X$ to distributions over actions. \\

In RL, the goal is to find an optimal policy $\pi^*$ that maximizes the cumulative expected return starting from an initial state $x_0$. In this paper, the goal is to learn a policy that generalizes well across related environments. \\

An \textbf{equivalence relation}, $E \subset S \times S$, is a bisimulation relation if wherever 2 states, $(s,t) \in E$, are considered bisimilar (denoted $s \sim t$) if the following properties hold:

\begin{enumerate}
    \item They have equal immediate rewards: 
    \begin{itemize}
        \item $\forall a \in A, R(s,a) = R(t,a)$
    \end{itemize}
    \item Transition with equal probability to bisimulation equivalence classes:
    \begin{itemize}
        \item $\forall a \in A, \forall c \in S_E, P(s,a)(c) = \sum_{s'\in c} P(s,a)(s') = P(t,a)(c)$
    \end{itemize}
\end{enumerate}

In other words, two states are bisimilar if they have similar expected rewards and dynamics. In the illustration (figure \ref{fig:BisimulationEquivalence}), we look at the system where a bisimualtion equivalence relation would collapse the 8 states on the left into an equivalent 4 state MDP to the right.

\begin{figure}[h]
    \centering
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width = \linewidth]{ATML/Introduction/image4.png}
        \caption{8 states}
        \label{}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width = \linewidth]{ATML/Introduction/image5.png}
        \caption{Collapsed 4 states}
        \label{}
    \end{subfigure}
    \caption{Bisimulation equivalence relation}
    \label{fig:BisimulationEquivalence}
\end{figure}

Getting the exact equivalence would be quite tricky in every case, so we try to find a metric yielding a smoother notion of similarity than equivalence relation. The bisimulation metrics are a generalization of bisimualtion relations.\\ 

Let $M$ be all pseudo-metrics of $S$, ie, all metrics $d \in M $ where $\forall (s,t) \in S, d(s,t)=0$.  A fixed point $d_{\sim}$, and this fixed point is a bisimulation metric:

\begin{equation}
    F(d)(s,a) = max_{a \in A} [|R(s,a) - R(t,a)| + \gamma \mathcal{W}_1 (d)(P(s,a),P(t,a))]
\end{equation}

where 
\begin{itemize}
    \item $s,t \in S$ are two states in the MDP.
    \item A is the action space.
    \item $R:S \times A \rightarrow R$ is the reward function.
    \item $P:S \times A \rightarrow \Delta(S)$ is the transition function.
    \item $\mathcal{W}_1 (d)$ is the Wasserstein distance between two probability distributions under a state metric $d$.
\end{itemize}

A very nice theoretical property of such metric is that the bisimulation distance between two states is an upper-bound on their optimal value difference: $|V^*(s)−V^*(t)| \leq d \sim (s,t)$. 

\begin{equation}\label{eq:BisimulationDistance}
    d_{\pi}(x,y) = |R^{\pi}(x) - R^{\pi}(x)| + \gamma \mathcal{W}_1(d_{\pi}) (P^{\pi}(\cdot | x), (P^{\pi}(\cdot | y)) 
\end{equation}

As it can be seen from equation \ref{eq:BisimulationDistance}, the bisimulation metric is a recursive equation with two terms. The first term calculates the reward difference, while the second term captures the long-term discounted future reward difference. Moreover, it defines the distance between two states x and y as the distance between expected rewards obtained when following policy π. The second term uses the Wasserstein-1 distance of the metric $d_{\pi}$. It quantifies the distance for the next state distributions following the policy $\pi$ at states x and y. A very easy illustration (\ref{fig:SandPile}) is to think of probability distributions as piles of sand. You can look at the sand pile as a probability mass. Then, the Wasserstein-1 distance calculates the dissimilarities between these piles of sand in terms of how much and how far we need to move the sand in order to transfer one pile into the other.

\begin{figure}[h]
    \centering
    \includegraphics[width = 0.5\linewidth]{ATML/Introduction/image6.png}
    \caption{Sand Pile analogy of Wasserstein metric}
    \label{fig:SandPile}
\end{figure}

There are a few drawbacks of using reward difference in bisimulation for measuring state similarities:

\begin{enumerate}
    \item Two MDPs can have the same policies/behaviours but different expected rewards. For example, consider the two MDPs shown in figure \ref{fig:TwoMDP}. Once the agent has taken the action to eat cake, the agent should stop, because eating cake is good, but eating too much cake is bad. The rewards are different in these two MDPs. For certain values of these rewards, the bisimulation distance captures correct state similarity.
\begin{figure}[h]
    \centering
    \includegraphics[width = 0.5\linewidth]{ATML/Introduction/image7.png}
    \caption{Same policies different rewards}
    \label{fig:TwoMDP}
\end{figure}
    \item Two states can have the same long term expected reward but different optimal policies. For instance,  figure \ref{fig:ThreeJump} illustrates three jumping tasks with different positions of the obstacles. The three states with a yellow boundary have the same expected rewards, which means that the bisimulation value is 0. However, as illustrated with the little arrow, the optimal action in the first task in the highlighted is to jump, while in the other tasks the optimal action is to go right.
\begin{figure}[h]
    \centering
    \includegraphics[width = 0.5\linewidth]{ATML/Introduction/image8.png}
    \caption{Same reward different policies}
    \label{fig:ThreeJump}
\end{figure}
    \item Pessimistic: the maximum is considering the worst possible case
    \item Computational expense: Bisimulation metrics have been solved using dynamic programming, but as this requires updating the metric estimate for all state-action pairs at every iteration, it is computationally expensive. This means computing the Wasserstein distance $|S|\times|A|$ times at each iteration.
    \item Full state enumerability: Existing methods for computing/approximating bisimulation metrics require full state enumerability (countability).
\end{enumerate}

Therefore, to address these issues, in this paper, the $\pi$-bisimulation metric, which is based on the bisimulation metric, is being used. In contrast to bisimulation metrics, which is built on reward differences, $\pi$-bisimulation metrics are built on differences in optimal policies. The absolute reward differences are replaced by policy differences. Furthermore, as the goal is to perform well in previously unseen environments, we are interested in optimal behaviour, and so, we use $\pi^*$ as the grounding policy. This yields Policy Similarity Metric (PSM).

\begin{equation}
    d_{\pi}(x,y) = DIST(\pi^*(x), \pi^*(y)) + \gamma \mathcal{W}_1(d^*) (P^{\pi^*}(\cdot | x), (P^{\pi^*}(\cdot | y)) 
\end{equation}

In PSM, states are considered similar when the optimal policies in these states as well as future states are similar. The DIST term captures the local optimal behaviour differences, while the Wasserstein term captures the long-term optimal behaviour differences. Moreover, the discount factor $\gamma$ assigns the weights.

Now, take into consideration transferring an optimal policy from one to another environment using PSM. We do this by finding the nearest neighbour x for each state y. This nearest neighbour has the minimum PSM distance to y. To define the transfer policy at y, the policy at x can be used. If we use this nearest neighbour transfer scheme, we have the advantage of an upper bound on the suboptimality of policies or optimal policies transferred from one environment to another by PSM, in contrast to bisimulation metric where we do not have such feature.  This illustrates how PSM provides a way of lifting generalization across inputs as a supervised learning problem to generalization across environments.

As we aim to achieve a good generalization, we learn policy similarity embeddings that encode this PSM. This is done by adapting SimCLR, which is a constructive method for learning embeddings of image inputs.

\section{Policy Similarity Embeddings - PSEs}


\end{document}
